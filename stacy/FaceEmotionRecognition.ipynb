{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fshnkarimi/FaceEmotionRecognition/blob/main/FaceEmotionRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-u-N_G3DBgm",
    "outputId": "10ee8c52-a26a-480b-8d28-67c619148bc0"
   },
   "outputs": [],
   "source": [
    "!pip install FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QgiDXsBZH6FQ"
   },
   "outputs": [],
   "source": [
    "from fer import Video\n",
    "from fer import FER\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "# Put in the location of the video file that has to be processed\n",
    "location_videofile = \"/content/afshin_face.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YGcF2hjIIdQv"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Video file not found at C:\\content\\afshin_face.mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m face_detector \u001b[38;5;241m=\u001b[39m FER(mtcnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Input the video for processing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m input_video \u001b[38;5;241m=\u001b[39m \u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_videofile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\fer\\classes.py:38\u001b[0m, in \u001b[0;36mVideo.__init__\u001b[1;34m(self, video_file, outdir, first_face_only, tempfile)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     26\u001b[0m     video_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     tempfile: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m ):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124;03m\"\"\"Video class for extracting and saving frames for emotion detection.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    :param video_file - str\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    :param outdir - str\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    :param tempfile - str\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(video_file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo file not found at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     39\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(video_file)\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_file)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(outdir):\n",
      "\u001b[1;31mAssertionError\u001b[0m: Video file not found at C:\\content\\afshin_face.mp4"
     ]
    }
   ],
   "source": [
    "# But the Face detection detector\n",
    "face_detector = FER(mtcnn=True)\n",
    "# Input the video for processing\n",
    "input_video = Video(location_videofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cca_EbI2IfkX",
    "outputId": "0cb32852-afb2-4f5f-c7ad-e642f911f93d"
   },
   "outputs": [],
   "source": [
    "# The Analyze() function will run analysis on every frame of the input video. \n",
    "# It will create a rectangular box around every image and show the emotion values next to that.\n",
    "# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
    "processing_data = input_video.analyze(face_detector, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "PAKSX3elJBLM",
    "outputId": "6838a0b0-74d6-4139-cf7d-9000294cdec4"
   },
   "outputs": [],
   "source": [
    "# We will now convert the analysed information into a dataframe.\n",
    "# This will help us import the data as a .CSV file to perform analysis over it later\n",
    "vid_df = input_video.to_pandas(processing_data)\n",
    "vid_df = input_video.get_first_face(vid_df)\n",
    "vid_df = input_video.get_emotions(vid_df)\n",
    "\n",
    "# Plotting the emotions against time in the video\n",
    "pltfig = vid_df.plot(figsize=(20, 8), fontsize=16).get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "eFHrMCH9JIbY",
    "outputId": "6466288c-5508-457f-927b-65fa7eab972a"
   },
   "outputs": [],
   "source": [
    "# We will now work on the dataframe to extract which emotion was prominent in the video\n",
    "angry = sum(vid_df.angry)\n",
    "disgust = sum(vid_df.disgust)\n",
    "fear = sum(vid_df.fear)\n",
    "happy = sum(vid_df.happy)\n",
    "sad = sum(vid_df.sad)\n",
    "surprise = sum(vid_df.surprise)\n",
    "neutral = sum(vid_df.neutral)\n",
    "\n",
    "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "\n",
    "score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
    "score_comparisons['Emotion Value from the Video'] = emotions_values\n",
    "score_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BwUMxOD8Vkv3",
    "outputId": "6492ac2e-86a0-489f-8f59-ec69b3e9039c"
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "path=\"/content/output/afshin_face_output.mp4\" \n",
    "\n",
    "clip=VideoFileClip(path)\n",
    "clip.ipython_display(width=560, maxduration=90)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHHu+QytOBi/9+P9IwjcC+",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "FaceEmotionRecognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
